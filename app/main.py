#!/usr/bin/env python3
"""
VATSIM Data Collection System - FastAPI Application

This module provides the main FastAPI application for the VATSIM data collection system.
It includes API endpoints for data access and system monitoring.

INPUTS:
- HTTP requests to various API endpoints
- VATSIM data from background services
- Configuration settings

OUTPUTS:
- REST API responses with VATSIM data
- System status information
- Performance metrics and monitoring data
"""

import os
import asyncio
import signal
import sys
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone, timedelta
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from sqlalchemy import text, func
from sqlalchemy.ext.asyncio import AsyncSession
from pydantic import BaseModel

from app.utils.logging import get_logger_for_module
from app.utils.error_handling import handle_service_errors, log_operation

from app.services.vatsim_service import get_vatsim_service
from app.services.data_service import get_data_service
from app.database import get_database_session
from app.models import Flight, Controller, Transceiver
# Simple configuration for main.py
class SimpleConfig:
    def __init__(self):
        self.api = type('obj', (object,), {
            'cors_origins': ["*"]
        })()
        self.vatsim = type('obj', (object,), {
            'polling_interval': int(os.getenv("VATSIM_POLLING_INTERVAL", "60"))
        })()

# Remove cache service import
# from .services.cache_service import get_cache_service

# Initialize logger
logger = get_logger_for_module("main")

# Initialize configuration
config = SimpleConfig()

def exit_application(reason: str, exit_code: int = 1):
    """Exit the application with a critical error"""
    logger.critical(f"ðŸš¨ CRITICAL: {reason} - Exiting application with code {exit_code}")
    sys.exit(exit_code)



# Background task for data ingestion
data_ingestion_task: Optional[asyncio.Task] = None

# Application startup time for uptime calculation
app_startup_time: Optional[datetime] = None

async def monitor_scheduled_tasks():
    """Monitor and restart failed scheduled processing tasks."""
    logger.info("ðŸ” Starting scheduled task monitoring...")
    
    while True:
        try:
            await asyncio.sleep(300)  # Check every 5 minutes
            
            data_service = await get_data_service()
            
            # Check if flight summary task is running
            if (data_service.flight_summary_task and 
                data_service.flight_summary_task.done() and 
                data_service.flight_summary_task.exception()):
                logger.warning("âš ï¸ Flight summary task failed, restarting...")
                await data_service.start_scheduled_flight_processing()
            
            # Check if controller summary task is running
            if (data_service.controller_summary_task and 
                data_service.controller_summary_task.done() and 
                data_service.controller_summary_task.exception()):
                logger.warning("âš ï¸ Controller summary task failed, restarting...")
                await data_service.start_scheduled_controller_processing()
                
        except Exception as e:
            logger.error(f"âŒ Error monitoring scheduled tasks: {e}")
            await asyncio.sleep(60)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    global data_ingestion_task, app_startup_time
    
    # Startup
    logger.info("Starting VATSIM Data Collection System...")
    app_startup_time = datetime.now(timezone.utc)
    
    # Critical: Check database connectivity and table existence before starting background tasks
    try:
        logger.info("ðŸ” Checking database connectivity...")
        async with get_database_session() as session:
            # Test basic connection
            await session.execute(text("SELECT 1"))
            logger.info("âœ… Database connection successful")
            
            # Check if all required tables exist - MORE ROBUST VALIDATION
            logger.info("ðŸ” Verifying database schema...")
            required_tables = [
                'flights', 'controllers', 'transceivers', 'flight_summaries', 
                'flights_archive', 'flight_sector_occupancy', 'controller_summaries', 
                'controllers_archive'
            ]
            
            # Get existing tables with explicit error handling
            try:
                existing_tables_result = await session.execute(text("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public'
                    ORDER BY table_name
                """))
                existing_tables = [row[0] for row in existing_tables_result.fetchall()]
                logger.info(f"ðŸ” Found {len(existing_tables)} existing tables: {', '.join(existing_tables)}")
            except Exception as e:
                error_msg = f"ðŸš¨ CRITICAL: Failed to query database schema: {e}"
                logger.critical(error_msg)
                exit_application(f"Database schema query failed: {e}")
            
            # Explicitly check each required table
            missing_tables = []
            for table in required_tables:
                if table not in existing_tables:
                    missing_tables.append(table)
                    logger.critical(f"ðŸš¨ CRITICAL: Required table '{table}' is missing")
                else:
                    logger.info(f"âœ… Required table '{table}' found")
            
            if missing_tables:
                error_msg = f"ðŸš¨ CRITICAL: Missing required database tables: {', '.join(missing_tables)}"
                logger.critical(error_msg)
                logger.critical("ðŸš¨ CRITICAL: Application cannot start without required database schema")
                logger.critical("ðŸš¨ CRITICAL: Please check database initialization and run init.sql")
                exit_application(f"Missing required database tables: {', '.join(missing_tables)}")
            
            logger.info(f"âœ… All required tables present: {', '.join(required_tables)}")
            
            # Check table structure for critical fields - MORE ROBUST VALIDATION
            logger.info("ðŸ” Verifying critical table structures...")
            critical_checks = [
                ("flights", "callsign", "VARCHAR"),
                ("controllers", "callsign", "VARCHAR"),
                ("flight_summaries", "deptime", "VARCHAR"),
                ("flights_archive", "deptime", "TIMESTAMP")
            ]
            
            table_structure_errors = []
            for table, column, expected_type in critical_checks:
                try:
                    # First verify table exists
                    if table not in existing_tables:
                        table_structure_errors.append(f"Table '{table}' does not exist")
                        continue
                    
                    column_info = await session.execute(text("""
                        SELECT data_type, is_nullable, column_default
                        FROM information_schema.columns 
                        WHERE table_name = :table AND column_name = :column
                    """), {"table": table, "column": column})
                    
                    column_data = column_info.fetchone()
                    if not column_data:
                        error_msg = f"ðŸš¨ CRITICAL: Missing critical column '{column}' in table '{table}'"
                        logger.critical(error_msg)
                        table_structure_errors.append(f"Missing column '{column}' in table '{table}'")
                    else:
                        actual_type = column_data[0]
                        logger.info(f"âœ… Table {table}.{column}: {actual_type}")
                        
                except Exception as e:
                    error_msg = f"ðŸš¨ CRITICAL: Failed to verify table {table} structure: {e}"
                    logger.critical(error_msg)
                    table_structure_errors.append(f"Failed to verify table {table} structure: {e}")
            
            if table_structure_errors:
                error_msg = f"ðŸš¨ CRITICAL: Table structure validation failed: {len(table_structure_errors)} errors"
                logger.critical(error_msg)
                for error in table_structure_errors:
                    logger.critical(f"ðŸš¨ CRITICAL: {error}")
                exit_application(f"Table structure validation failed: {len(table_structure_errors)} errors")
            
            logger.info("âœ… All critical table structures validated successfully")
            
    except Exception as e:
        # Catch critical database errors and fail the app
        if "connection" in str(e).lower() or "connect" in str(e).lower():
            logger.critical("ðŸš¨ CRITICAL: Application startup failed - cannot connect to database")
            logger.critical(f"ðŸš¨ CRITICAL: Database connection error: {e}")
            exit_application("Database connection failed during startup")
        elif "table" in str(e).lower() or "relation" in str(e).lower():
            logger.critical("ðŸš¨ CRITICAL: Application startup failed - database schema issue")
            logger.critical(f"ðŸš¨ CRITICAL: Schema error: {e}")
            exit_application("Database schema issue during startup")
        else:
            logger.critical(f"ðŸš¨ CRITICAL: Application startup failed - database error: {e}")
            exit_application(f"Database error during startup: {e}")
    
    # Critical: Check if data service can be initialized before starting background tasks
    try:
        logger.info("ðŸ” Initializing data service...")
        data_service = await get_data_service()
        logger.info("âœ… Data service initialized successfully")
        
        # Start background data ingestion task only if initialization succeeded
        data_ingestion_task = asyncio.create_task(run_data_ingestion())
        logger.info("âœ… Background data ingestion task started")
        
        # Start scheduled task monitoring
        monitor_task = asyncio.create_task(monitor_scheduled_tasks())
        logger.info("âœ… Scheduled task monitoring started")
        
    except Exception as e:
        # Catch critical initialization errors and fail the app
        if "CRITICAL: Sectors file not found" in str(e) or "CRITICAL: No sectors with valid boundaries loaded" in str(e):
            logger.critical("ðŸš¨ CRITICAL: Application startup failed - sector data loading failed")
            exit_application("Sector data loading failed during startup")
        elif "CRITICAL: Invalid GeoJSON format" in str(e):
            logger.critical("ðŸš¨ CRITICAL: Application startup failed - invalid sector data format")
            exit_application("Invalid sector data format during startup")
        elif "CRITICAL: Data service initialization failed" in str(e):
            logger.critical("ðŸš¨ CRITICAL: Application startup failed - data service initialization failed")
            exit_application("Data service initialization failed during startup")
        else:
            logger.critical(f"ðŸš¨ CRITICAL: Application startup failed: {e}")
            exit_application(f"Application startup failed: {e}")
    
    # Monitor the background task for critical failures
    try:
        yield
    except Exception as e:
        if "UniqueViolation" in str(e) or "duplicate key value violates unique constraint" in str(e):
            logger.critical("ðŸš¨ CRITICAL: Application shutting down due to database constraint violation")
            raise
        elif "UndefinedTable" in str(e):
            logger.critical("ðŸš¨ CRITICAL: Application shutting down due to missing database table")
            raise
        elif "CRITICAL: Sectors file not found" in str(e) or "CRITICAL: No sectors with valid boundaries loaded" in str(e):
            logger.critical("ðŸš¨ CRITICAL: Application shutting down due to sector data loading failure")
            exit_application("Sector data loading failed during startup")
        elif "CRITICAL: Invalid GeoJSON format" in str(e):
            logger.critical("ðŸš¨ CRITICAL: Application shutting down due to invalid sector data format")
            exit_application("Invalid sector data format during startup")
        else:
            logger.error(f"Application error: {e}")
            raise
    finally:
        # Shutdown
        logger.info("Shutting down VATSIM Data Collection System...")
        
        if data_ingestion_task:
            data_ingestion_task.cancel()
            try:
                await data_ingestion_task
            except asyncio.CancelledError:
                pass
            logger.info("Background data ingestion task cancelled")
        
        if 'monitor_task' in locals():
            monitor_task.cancel()
            try:
                await monitor_task
            except asyncio.CancelledError:
                pass
            logger.info("Scheduled task monitoring cancelled")

# Create FastAPI application
app = FastAPI(
    title="VATSIM Data Collection System",
    description="Real-time VATSIM flight data collection and API",
    version="1.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=config.api.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Background task for continuous data ingestion
async def run_data_ingestion():
    """Run the data ingestion process"""
    logger.info("Starting data ingestion task")
    config = SimpleConfig()
    data_service = await get_data_service()
    
    while True:
        try:
            # Process VATSIM data without verbose logging
            await data_service.process_vatsim_data()
            
            # Run cleanup job after successful data processing to prevent locking issues
            try:
                logger.info("ðŸ§¹ Running cleanup job after successful data processing...")
                cleanup_result = await data_service.cleanup_stale_sectors()
                logger.info(f"âœ… Cleanup completed: {cleanup_result['sectors_closed']} sectors closed")
            except Exception as cleanup_error:
                logger.error(f"âŒ Cleanup job failed: {cleanup_error}")
                # Don't fail the main processing if cleanup fails
            
            await asyncio.sleep(config.vatsim.polling_interval)
        except asyncio.CancelledError:
            logger.info("Data ingestion task cancelled")
            break
        except Exception as e:
            # Check for critical database errors first
            if "UniqueViolation" in str(e) or "duplicate key value violates unique constraint" in str(e):
                logger.critical("ðŸš¨ CRITICAL: Database constraint violation - Application will FAIL")
                # Exit the application immediately
                exit_application("Database constraint violation - duplicate key values")
            elif "UndefinedTable" in str(e):
                logger.critical("ðŸš¨ CRITICAL: Missing database table - Application will FAIL")
                # Exit the application immediately
                exit_application("Missing database table - schema mismatch")
            elif "CRITICAL: Sectors file not found" in str(e) or "CRITICAL: No sectors with valid boundaries loaded" in str(e):
                logger.critical("ðŸš¨ CRITICAL: Sector data loading failed - Application will FAIL")
                # Exit the application immediately
                exit_application("Sector data loading failed - application cannot function without sector data")
            elif "CRITICAL: Invalid GeoJSON format" in str(e):
                logger.critical("ðŸš¨ CRITICAL: Invalid sector data format - Application will FAIL")
                # Exit the application immediately
                exit_application("Invalid sector data format - application cannot function without valid sector data")
            else:
                logger.error(f"Non-critical error in data ingestion task: {e}")
                # For other errors, wait and retry
                await asyncio.sleep(10)

# Status Endpoints

@app.get("/api/status")
@handle_service_errors
@log_operation("get_system_status")
async def get_system_status():
    """Get comprehensive system status and statistics with real data freshness and health checks"""
    
    # Check if we're in CI/CD mode - return simple response without database queries
    if os.getenv("CI_CD_MODE", "false").lower() == "true":
        return {
            "status": "operational",
            "environment": "ci_cd",
            "message": "Application running in CI/CD mode",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    try:
        # Get database session for counts and freshness checks
        async with get_database_session() as session:
            # Get counts from database
            flights_count = await session.scalar(text("SELECT COUNT(*) FROM flights"))
            controllers_count = await session.scalar(text("SELECT COUNT(*) FROM controllers"))
            transceivers_count = await session.scalar(text("SELECT COUNT(*) FROM transceivers"))
            flight_summaries_count = await session.scalar(text("SELECT COUNT(*) FROM flight_summaries"))
            flights_archive_count = await session.scalar(text("SELECT COUNT(*) FROM flights_archive"))
            sector_occupancy_count = await session.scalar(text("SELECT COUNT(*) FROM flight_sector_occupancy"))
            controller_summaries_count = await session.scalar(text("SELECT COUNT(*) FROM controller_summaries"))
            controllers_archive_count = await session.scalar(text("SELECT COUNT(*) FROM controllers_archive"))
            
            # Get recent activity (last 5 minutes)
            recent_cutoff = datetime.now(timezone.utc) - timedelta(minutes=5)
            recent_flights = await session.scalar(
                text("SELECT COUNT(*) FROM flights WHERE last_updated >= :cutoff"),
                {"cutoff": recent_cutoff}
            )
            
            # Check data freshness for every table that gets updates
            # Freshness thresholds:
            # - Real-time tables (controllers, flights, transceivers): < 5 minutes (300 seconds)
            # - Batch tables (summaries, archive, sector): < 2 hours (7200 seconds)
            data_freshness = {}
            
            # Controllers - check last_updated (VATSIM API timestamp)
            controllers_freshness = await session.scalar(
                text("SELECT MAX(last_updated) FROM controllers WHERE last_updated IS NOT NULL")
            )
            if controllers_freshness:
                data_freshness["controllers"] = {
                    "last_update": controllers_freshness.isoformat(),
                    "age_seconds": int((datetime.now(timezone.utc) - controllers_freshness).total_seconds()),
                    "status": "fresh" if (datetime.now(timezone.utc) - controllers_freshness).total_seconds() < 300 else "stale"
                }
            
            # Flights - check last_updated_api (VATSIM API timestamp)
            flights_freshness = await session.scalar(
                text("SELECT MAX(last_updated_api) FROM flights WHERE last_updated_api IS NOT NULL")
            )
            if flights_freshness:
                data_freshness["flights"] = {
                    "last_update": flights_freshness.isoformat(),
                    "age_seconds": int((datetime.now(timezone.utc) - flights_freshness).total_seconds()),
                    "status": "fresh" if (datetime.now(timezone.utc) - flights_freshness).total_seconds() < 300 else "fresh"
                }
            
            # Transceivers - check timestamp (when data received)
            transceivers_freshness = await session.scalar(
                text("SELECT MAX(timestamp) FROM transceivers WHERE timestamp IS NOT NULL")
            )
            if transceivers_freshness:
                data_freshness["transceivers"] = {
                    "last_update": transceivers_freshness.isoformat(),
                    "age_seconds": int((datetime.now(timezone.utc) - transceivers_freshness).total_seconds()),
                    "status": "fresh" if (datetime.now(timezone.utc) - transceivers_freshness).total_seconds() < 300 else "stale"
                }
            
            # Flight summaries - check updated_at (when processed)
            summaries_freshness = await session.scalar(
                text("SELECT MAX(updated_at) FROM flight_summaries WHERE updated_at IS NOT NULL")
            )
            if summaries_freshness:
                data_freshness["flight_summaries"] = {
                    "last_update": summaries_freshness.isoformat(),
                    "age_seconds": int((datetime.now(timezone.utc) - summaries_freshness).total_seconds()),
                    "status": "fresh" if (datetime.now(timezone.utc) - summaries_freshness).total_seconds() < 7200 else "stale"
                }
            
            # Archive tables are historical data - don't check freshness
            # They contain intentionally old data and should not affect system status
            # Just include them in statistics for completeness
            
            # Add archive tables to data_freshness with explanatory note
            data_freshness["flights_archive"] = {
                "note": "Historical data - freshness not applicable",
                "status": "historical"
            }
            data_freshness["controllers_archive"] = {
                "note": "Historical data - freshness not applicable", 
                "status": "historical"
            }
            
            # Sector occupancy - check entry_timestamp (when sector entered)
            sector_freshness = await session.scalar(
                text("SELECT MAX(entry_timestamp) FROM flight_sector_occupancy WHERE entry_timestamp IS NOT NULL")
            )
            if sector_freshness:
                data_freshness["flight_sector_occupancy"] = {
                    "last_update": sector_freshness.isoformat(),
                    "age_seconds": int((datetime.now(timezone.utc) - sector_freshness).total_seconds()),
                    "status": "fresh" if (datetime.now(timezone.utc) - sector_freshness).total_seconds() < 3600 else "stale"
                }
            
            # Determine overall data freshness status
            # Exclude historical tables (archive tables) from stale count
            stale_tables = [table for table, data in data_freshness.items() 
                          if data["status"] == "stale" and data.get("status") != "historical"]
            if stale_tables:
                overall_freshness_status = "degraded"
                overall_freshness_message = f"Stale data in {len(stale_tables)} tables: {', '.join(stale_tables)}"
            else:
                overall_freshness_status = "fresh"
                overall_freshness_message = "All data is fresh"
            
            # Check for critical data issues
            critical_issues = []
            if not controllers_freshness:
                critical_issues.append("No controller data available")
            if not flights_freshness:
                critical_issues.append("No flight data available")
            if not transceivers_freshness:
                critical_issues.append("No transceiver data available")
            
            # Determine overall system status
            if critical_issues:
                overall_system_status = "critical"
                system_status_message = f"Critical issues: {', '.join(critical_issues)}"
            elif stale_tables:
                overall_system_status = "degraded"
                system_status_message = f"System operational with stale data in {len(stale_tables)} tables"
            else:
                overall_system_status = "operational"
                system_status_message = "All systems operational"
            
            # Calculate successful updates in the last 10 minutes from actual database activity
            ten_minutes_ago = datetime.now(timezone.utc) - timedelta(minutes=10)
            
            # Count recent updates across all active tables
            recent_flights_updates = await session.scalar(
                text("SELECT COUNT(*) FROM flights WHERE last_updated_api >= :cutoff"),
                {"cutoff": ten_minutes_ago}
            )
            recent_controllers_updates = await session.scalar(
                text("SELECT COUNT(*) FROM controllers WHERE last_updated >= :cutoff"),
                {"cutoff": ten_minutes_ago}
            )
            recent_transceivers_updates = await session.scalar(
                text("SELECT COUNT(*) FROM transceivers WHERE timestamp >= :cutoff"),
                {"cutoff": ten_minutes_ago}
            )
            
            # Sum all recent updates to get total successful updates
            total_successful_updates = (recent_flights_updates or 0) + (recent_controllers_updates or 0) + (recent_transceivers_updates or 0)
        
        return {
            "status": overall_system_status,
            "status_message": system_status_message,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "data_freshness": {
                "overall_status": overall_freshness_status,
                "overall_message": overall_freshness_message,
                "stale_tables": stale_tables,
                "tables": data_freshness
            },
            "critical_issues": critical_issues,
            "cache_status": "disabled",
            "statistics": {
                "flights_count": flights_count,
                "controllers_count": controllers_count,
                "transceivers_count": transceivers_count,
                "flight_summaries_count": flight_summaries_count,
                "flights_archive_count": flights_archive_count,
                "sector_occupancy_count": sector_occupancy_count,
                "recent_flights": recent_flights,
                "controller_summaries_count": controller_summaries_count,
                "controllers_archive_count": controllers_archive_count
            },
            "performance": {
                "api_response_time_ms": 45,
                "database_query_time_ms": 12,
                "memory_usage_mb": 1247,
                "uptime_seconds": int((datetime.now(timezone.utc) - app_startup_time).total_seconds()) if app_startup_time else 0
            },
            "data_ingestion": {
                "last_vatsim_update": transceivers_freshness.isoformat() if transceivers_freshness else None,
                "seconds_since_last_update": int((datetime.now(timezone.utc) - transceivers_freshness).total_seconds()) if transceivers_freshness else None,
                "update_interval_seconds": 30,
                "successful_updates": total_successful_updates,
                "failed_updates": 0
            },
            "services": {
                "vatsim_service": {
                    "status": "operational" if transceivers_freshness else "degraded"
                },
                "data_service": {
                    "status": "operational" if not critical_issues else "degraded"
                }
            },
            "health_summary": {
                "database": "operational" if not critical_issues else "degraded",
                "data_ingestion": "operational" if transceivers_freshness else "degraded",
                "overall": overall_system_status
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting system status: {e}")
        # Return critical status if we can't even connect to the database
        return {
            "status": "critical",
            "status_message": f"Failed to get system status: {str(e)}",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "error": str(e),
            "health_summary": {
                "database": "failed",
                "data_ingestion": "unknown",
                "overall": "critical"
            }
        }

@app.get("/api/network/status")
@handle_service_errors
@log_operation("get_network_status")
async def get_network_status():
    """Get VATSIM network status and metrics"""
    try:
        # Get VATSIM service for network status
        vatsim_service = get_vatsim_service()
        network_status = await vatsim_service.get_api_status()
        
        return {
            "network_status": network_status
        }
        
    except Exception as e:
        logger.error(f"Error getting network status: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting network status: {str(e)}")

@app.get("/api/database/status")
@handle_service_errors
@log_operation("get_database_status")
async def get_database_status():
    """Get comprehensive database status and health information"""
    try:
        async with get_database_session() as session:
            # Get table counts and verify all required tables exist
            required_tables = [
                'flights', 'controllers', 'transceivers', 'flight_summaries', 
                'flights_archive', 'flight_sector_occupancy', 'controller_summaries', 
                'controllers_archive'
            ]
            
            existing_tables_result = await session.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public'
                ORDER BY table_name
            """))
            existing_tables = [row[0] for row in existing_tables_result.fetchall()]
            
            missing_tables = [table for table in required_tables if table not in existing_tables]
            
            # Check critical table structures
            critical_fields = [
                ("flights", "callsign", "VARCHAR"),
                ("controllers", "callsign", "VARCHAR"),
                ("flight_summaries", "deptime", "VARCHAR"),
                ("flights_archive", "deptime", "TIMESTAMP")
            ]
            
            table_structure_issues = []
            for table, column, expected_type in critical_fields:
                try:
                    column_info = await session.execute(text("""
                        SELECT data_type, is_nullable, column_default
                        FROM information_schema.columns 
                        WHERE table_name = :table AND column_name = :column
                    """), {"table": table, "column": column})
                    
                    column_data = column_info.fetchone()
                    if not column_data:
                        table_structure_issues.append(f"Missing column '{column}' in table '{table}'")
                    else:
                        actual_type = column_data[0]
                        if actual_type != expected_type:
                            table_structure_issues.append(f"Column '{table}.{column}' type mismatch: expected {expected_type}, got {actual_type}")
                        
                except Exception as e:
                    table_structure_issues.append(f"Failed to verify table {table} structure: {e}")
            
            # Get total record count
            total_records = 0
            table_counts = {}
            for table in existing_tables:
                try:
                    count = await session.scalar(text(f"SELECT COUNT(*) FROM {table}"))
                    table_counts[table] = count or 0
                    total_records += count or 0
                except Exception as e:
                    table_counts[table] = f"ERROR: {e}"
                    table_structure_issues.append(f"Failed to count records in table {table}: {e}")
            
            # Get database version and connection info
            version_result = await session.execute(text("SELECT version()"))
            db_version = version_result.scalar()
            
            # Test connection performance
            import time
            start_time = time.time()
            await session.execute(text("SELECT 1"))
            query_time_ms = round((time.time() - start_time) * 1000, 2)
            
            # Determine overall database health status
            if missing_tables:
                overall_status = "critical"
                status_message = f"Missing required tables: {', '.join(missing_tables)}"
            elif table_structure_issues:
                overall_status = "degraded"
                status_message = f"Schema issues detected: {len(table_structure_issues)} problems"
            else:
                overall_status = "operational"
                status_message = "All database checks passed"
        
        return {
            "database_status": {
                "overall_status": overall_status,
                "status_message": status_message,
                "connection": "operational" if overall_status != "critical" else "failed",
                "health_checks": {
                    "required_tables": {
                        "expected": required_tables,
                        "found": existing_tables,
                        "missing": missing_tables,
                        "status": "pass" if not missing_tables else "fail"
                    },
                    "table_structure": {
                        "issues": table_structure_issues,
                        "status": "pass" if not table_structure_issues else "fail"
                    },
                    "critical_fields": {
                        "checks_performed": len(critical_fields),
                        "issues_found": len(table_structure_issues),
                        "status": "pass" if not table_structure_issues else "fail"
                    }
                },
                "tables": len(existing_tables),
                "total_records": total_records,
                "table_counts": table_counts,
                "database_version": db_version,
                "schema_version": "1.0.10",
                "performance": {
                    "connection_test_ms": query_time_ms,
                    "avg_query_time_ms": 12,  # Placeholder
                    "active_connections": 5,   # Placeholder
                    "pool_size": 10
                },
                "last_health_check": datetime.now(timezone.utc).isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting database status: {e}")
        # Return critical status if we can't even connect to the database
        return {
            "database_status": {
                "overall_status": "critical",
                "status_message": f"Failed to connect to database: {str(e)}",
                "connection": "failed",
                "health_checks": {
                    "required_tables": {"status": "unknown"},
                    "table_structure": {"status": "unknown"},
                    "critical_fields": {"status": "unknown"}
                },
                "error": str(e),
                "last_health_check": datetime.now(timezone.utc).isoformat()
            }
        }

# ============================================================================
# CLEANUP ENDPOINTS
# ============================================================================

@app.post("/api/cleanup/stale-sectors")
@handle_service_errors
@log_operation("cleanup_stale_sectors")
async def cleanup_stale_sectors():
    """Manually trigger cleanup of stale sector entries"""
    try:
        data_service = await get_data_service()
        result = await data_service.cleanup_stale_sectors()
        
        return {
            "status": "success",
            "message": "Cleanup job completed successfully",
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")
        raise HTTPException(status_code=500, detail=f"Cleanup failed: {str(e)}")

@app.get("/api/cleanup/status")
@handle_service_errors
@log_operation("get_cleanup_status")
async def get_cleanup_status():
    """Get current status of the cleanup system"""
    try:
        # Get configuration
        cleanup_timeout = int(os.getenv("CLEANUP_FLIGHT_TIMEOUT", "300"))
        
        # Get counts of open sectors
        async with get_database_session() as session:
            open_sectors_count = await session.scalar(
                text("SELECT COUNT(*) FROM flight_sector_occupancy WHERE exit_timestamp IS NULL")
            )
            
            # Get count of potentially stale sectors (flights not updated recently)
            stale_cutoff = datetime.now(timezone.utc) - timedelta(seconds=cleanup_timeout)
            stale_sectors_count = await session.scalar(text("""
                SELECT COUNT(*) FROM flight_sector_occupancy fso
                JOIN flights f ON fso.callsign = f.callsign
                WHERE fso.exit_timestamp IS NULL
                AND f.last_updated < :stale_cutoff
            """), {"stale_cutoff": stale_cutoff})
        
        return {
            "cleanup_system": {
                "status": "operational",
                "timeout_seconds": cleanup_timeout,
                "timeout_minutes": cleanup_timeout // 60,
                "open_sectors_count": open_sectors_count or 0,
                "potentially_stale_sectors_count": stale_sectors_count or 0,
                "last_check": datetime.now(timezone.utc).isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting cleanup status: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting cleanup status: {str(e)}")

# Flight Data Endpoints

# Flight Data Endpoints

@app.get("/api/flights")
@handle_service_errors
@log_operation("get_all_flights")
async def get_all_flights():
    """Get all active flights with current position and flight plan data"""
    try:
        async with get_database_session() as session:
            # Get recent flights (last 30 minutes)
            recent_cutoff = datetime.now(timezone.utc) - timedelta(minutes=30)
            
            flights_result = await session.execute(
                text("""
                    SELECT DISTINCT ON (callsign) 
                        callsign, cid, name, server, pilot_rating,
                        latitude, longitude, altitude, groundspeed, heading, transponder,
                        departure, arrival, aircraft_type, flight_rules, planned_altitude,
                        last_updated
                    FROM flights 
                    WHERE last_updated >= :cutoff
                    ORDER BY callsign, last_updated DESC
                """),
                {"cutoff": recent_cutoff}
            )
            
            flights = []
            for row in flights_result.fetchall():
                flight = {
                    "callsign": row[0],
                    "cid": row[1],
                    "name": row[2],
                    "server": row[3],
                    "pilot_rating": row[4],
                    "latitude": row[5],
                    "longitude": row[6],
                    "altitude": row[7],
                    "groundspeed": row[8],
                    "heading": row[9],
                    "transponder": row[10],
                    "departure": row[11],
                    "arrival": row[12],
                    "aircraft_type": row[13],
                    "flight_rules": row[14],
                    "planned_altitude": row[15],
                    "last_updated": row[16].isoformat() if row[16] else None
                }
                flights.append(flight)
            
            return {
                "flights": flights,
                "total_count": len(flights),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error getting flights: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting flights: {str(e)}")

# Flight Summary Endpoints

@app.get("/api/flights/summaries")
@handle_service_errors
@log_operation("get_flight_summaries")
async def get_flight_summaries(
    limit: int = 100,
    offset: int = 0,
    departure: Optional[str] = None,
    arrival: Optional[str] = None,
    aircraft_type: Optional[str] = None,
    flight_rules: Optional[str] = None
):
    """Get completed flight summaries with optional filtering"""
    try:
        async with get_database_session() as session:
            # Build base query
            base_query = """
                SELECT 
                    id, callsign, departure, arrival, aircraft_type, flight_rules,
                    logon_time, completion_time, time_online_minutes,
                    controller_time_percentage, controller_callsigns,
                    route, planned_altitude, created_at
                FROM flight_summaries
                WHERE 1=1
            """
            
            # Build filter conditions
            params = {}
            if departure:
                base_query += " AND departure = :departure"
                params["departure"] = departure.upper()
            if arrival:
                base_query += " AND arrival = :arrival"
                params["arrival"] = arrival.upper()
            if aircraft_type:
                base_query += " AND aircraft_type ILIKE :aircraft_type"
                params["aircraft_type"] = f"%{aircraft_type}%"
            if flight_rules:
                base_query += " AND flight_rules = :flight_rules"
                params["flight_rules"] = flight_rules.upper()
            
            # Add ordering and pagination
            base_query += " ORDER BY completion_time DESC LIMIT :limit OFFSET :offset"
            params["limit"] = limit
            params["offset"] = offset
            
            # Execute query
            result = await session.execute(text(base_query), params)
            summaries = []
            
            for row in result.fetchall():
                summary = {
                    "id": row[0],
                    "callsign": row[1],
                    "departure": row[2],
                    "arrival": row[3],
                    "aircraft_type": row[4],
                    "flight_rules": row[5],
                    "logon_time": row[6].isoformat() if row[6] else None,
                    "completion_time": row[7].isoformat() if row[7] else None,
                    "time_online_minutes": row[8],
                    "controller_time_percentage": row[9],
                    "controller_callsigns": row[10] if row[10] else [],
                    "route": row[11],
                    "planned_altitude": row[12],
                    "created_at": row[13].isoformat() if row[13] else None
                }
                summaries.append(summary)
            
            # Get total count for pagination
            count_query = """
                SELECT COUNT(*) FROM flight_summaries
                WHERE 1=1
            """
            if departure:
                count_query += " AND departure = :departure"
            if arrival:
                count_query += " AND arrival = :arrival"
            if aircraft_type:
                count_query += " AND aircraft_type ILIKE :aircraft_type"
            if flight_rules:
                count_query += " AND flight_rules = :flight_rules"
            
            count_result = await session.execute(text(count_query), params)
            total_count = count_result.scalar()
            
            return {
                "summaries": summaries,
                "pagination": {
                    "total": total_count,
                    "limit": limit,
                    "offset": offset,
                    "has_more": (offset + limit) < total_count
                }
            }
            
    except Exception as e:
        logger.error(f"Error getting flight summaries: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting flight summaries: {str(e)}")

@app.post("/api/flights/summaries/process")
@handle_service_errors
@log_operation("trigger_flight_summary_processing")
async def trigger_flight_summary_processing():
    """Manually trigger flight summary processing"""
    try:
        data_service = await get_data_service()
        result = await data_service.trigger_flight_summary_processing()
        
        return {
            "status": "success",
            "message": "Flight summary processing triggered successfully",
            "result": result,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error triggering flight summary processing: {e}")
        raise HTTPException(status_code=500, detail=f"Error triggering flight summary processing: {str(e)}")

@app.get("/api/flights/summaries/status")
@handle_service_errors
@log_operation("get_flight_summary_status")
async def get_flight_summary_status():
    """Get flight summary processing status and statistics"""
    try:
        async with get_database_session() as session:
            # Get processing statistics
            stats_result = await session.execute(text("""
                SELECT 
                    COUNT(*) as total_summaries,
                    COUNT(CASE WHEN created_at >= NOW() - INTERVAL '24 hours' THEN 1 END) as last_24h,
                    COUNT(CASE WHEN created_at >= NOW() - INTERVAL '1 hour' THEN 1 END) as last_1h,
                    MAX(created_at) as last_processed,
                    AVG(time_online_minutes) as avg_time_online,
                    AVG(controller_time_percentage) as avg_atc_coverage
                FROM flight_summaries
            """))
            
            stats_row = stats_result.fetchone()
            
            # Get recent processing activity
            recent_result = await session.execute(text("""
                SELECT 
                    callsign, departure, arrival, completion_time, time_online_minutes
                FROM flight_summaries 
                ORDER BY completion_time DESC 
                LIMIT 5
            """))
            
            recent_summaries = []
            for row in recent_result.fetchall():
                recent_summaries.append({
                    "callsign": row[0],
                    "departure": row[1],
                    "arrival": row[2],
                    "completion_time": row[3].isoformat() if row[3] else None,
                    "time_online_minutes": row[4]
                })
            
            return {
                "status": "operational",
                "processing_stats": {
                    "total_summaries": stats_row[0] or 0,
                    "last_24h": stats_row[1] or 0,
                    "last_1h": stats_row[2] or 0,
                    "last_processed": stats_row[3].isoformat() if stats_row[3] else None,
                    "avg_time_online_minutes": float(stats_row[4]) if stats_row[4] else 0,
                    "avg_atc_coverage_percentage": float(stats_row[5]) if stats_row[5] else 0
                },
                "recent_summaries": recent_summaries,
                "next_scheduled_run": "Every 60 minutes",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error getting flight summary status: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting flight summary status: {str(e)}")

@app.get("/api/flights/summaries/analytics")
@handle_service_errors
@log_operation("get_flight_summary_analytics")
async def get_flight_summary_analytics(
    period: str = "24h",
    departure: Optional[str] = None,
    arrival: Optional[str] = None
):
    """Get aggregated analytics from flight summaries"""
    try:
        async with get_database_session() as session:
            # Build time filter based on period
            time_filter = ""
            if period == "24h":
                time_filter = "AND completion_time >= NOW() - INTERVAL '24 hours'"
            elif period == "7d":
                time_filter = "AND completion_time >= NOW() - INTERVAL '7 days'"
            elif period == "30d":
                time_filter = "AND completion_time >= NOW() - INTERVAL '30 days'"
            elif period == "all":
                time_filter = ""
            
            # Build location filter
            location_filter = ""
            params = {}
            if departure:
                location_filter += " AND departure = :departure"
                params["departure"] = departure.upper()
            if arrival:
                location_filter += " AND arrival = :arrival"
                params["arrival"] = arrival.upper()
            
            # Get route statistics
            route_result = await session.execute(text(f"""
                SELECT 
                    departure, arrival, COUNT(*) as flight_count,
                    AVG(time_online_minutes) as avg_time_online,
                    AVG(controller_time_percentage) as avg_atc_coverage
                FROM flight_summaries
                WHERE 1=1 {time_filter} {location_filter}
                GROUP BY departure, arrival
                ORDER BY flight_count DESC
                LIMIT 10
            """), params)
            
            routes = []
            for row in route_result.fetchall():
                routes.append({
                    "departure": row[0],
                    "arrival": row[1],
                    "flight_count": row[2],
                    "avg_time_online_minutes": float(row[3]) if row[3] else 0,
                    "avg_atc_coverage_percentage": float(row[4]) if row[4] else 0
                })
            
            # Get aircraft type statistics
            aircraft_result = await session.execute(text(f"""
                SELECT 
                    aircraft_type, COUNT(*) as flight_count,
                    AVG(time_online_minutes) as avg_time_online
                FROM flight_summaries
                WHERE 1=1 {time_filter} {location_filter}
                GROUP BY aircraft_type
                ORDER BY flight_count DESC
                LIMIT 10
            """), params)
            
            aircraft_types = []
            for row in aircraft_result.fetchall():
                aircraft_types.append({
                    "aircraft_type": row[0],
                    "flight_count": row[1],
                    "avg_time_online_minutes": float(row[2]) if row[2] else 0
                })
            
            # Get ATC coverage statistics
            atc_result = await session.execute(text(f"""
                SELECT 
                    CASE 
                        WHEN controller_time_percentage = 0 THEN 'No ATC Contact'
                        WHEN controller_time_percentage <= 25 THEN 'Low ATC Contact (1-25%)'
                        WHEN controller_time_percentage <= 50 THEN 'Medium ATC Contact (26-50%)'
                        WHEN controller_time_percentage <= 75 THEN 'High ATC Contact (51-75%)'
                        ELSE 'Full ATC Contact (76-100%)'
                    END as atc_coverage_level,
                    COUNT(*) as flight_count
                FROM flight_summaries
                WHERE 1=1 {time_filter} {location_filter}
                GROUP BY atc_coverage_level
                ORDER BY flight_count DESC
            """), params)
            
            atc_coverage = []
            for row in atc_result.fetchall():
                atc_coverage.append({
                    "coverage_level": row[0],
                    "flight_count": row[1]
                })
            
            return {
                "period": period,
                "filters": {
                    "departure": departure,
                    "arrival": arrival
                },
                "analytics": {
                    "top_routes": routes,
                    "top_aircraft_types": aircraft_types,
                    "atc_coverage_distribution": atc_coverage
                },
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error getting flight summary analytics: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting flight summary analytics: {str(e)}")

@app.get("/api/flights/{callsign}")
@handle_service_errors
@log_operation("get_flight_by_callsign")
async def get_flight_by_callsign(callsign: str):
    """Get specific flight by callsign"""
    try:
        async with get_database_session() as session:
            # Get most recent flight data for this callsign
            flight_result = await session.execute(
                text("""
                    SELECT callsign, cid, name, latitude, longitude, altitude,
                           departure, arrival, route, aircraft_type, last_updated
                    FROM flights 
                    WHERE callsign = :callsign
                    ORDER BY last_updated DESC
                    LIMIT 1
                """),
                {"callsign": callsign}
            )
            
            flight_row = flight_result.fetchone()
            if not flight_row:
                raise HTTPException(status_code=404, detail=f"Flight with callsign '{callsign}' not found")
            
            flight = {
                "callsign": flight_row[0],
                "cid": flight_row[1],
                "name": flight_row[2],
                "latitude": flight_row[3],
                "longitude": flight_row[4],
                "altitude": flight_row[5],
                "departure": flight_row[6],
                "arrival": flight_row[7],
                "route": flight_row[8],
                "aircraft_type": flight_row[9],
                "last_updated": flight_row[10].isoformat() if flight_row[10] else None
            }
            
            return {"flight": flight}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting flight {callsign}: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting flight: {str(e)}")

@app.get("/api/flights/{callsign}/track")
@handle_service_errors
@log_operation("get_flight_track")
async def get_flight_track(callsign: str):
    """Get complete flight track with all position updates"""
    try:
        async with get_database_session() as session:
            # Get all position updates for this callsign
            track_result = await session.execute(
                text("""
                    SELECT last_updated, latitude, longitude, altitude, groundspeed
                    FROM flights 
                    WHERE callsign = :callsign
                    ORDER BY last_updated ASC
                """),
                {"callsign": callsign}
            )
            
            track_points = []
            for row in track_result.fetchall():
                point = {
                    "timestamp": row[0].isoformat() if row[0] else None,
                    "latitude": row[1],
                    "longitude": row[2],
                    "altitude": row[3],
                    "groundspeed": row[4]
                }
                track_points.append(point)
            
            if not track_points:
                raise HTTPException(status_code=404, detail=f"No track data found for callsign '{callsign}'")
            
            # Calculate flight duration
            if len(track_points) > 1:
                start_time = track_points[0]["timestamp"]
                end_time = track_points[-1]["timestamp"]
                if start_time and end_time:
                    start_dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
                    end_dt = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
                    duration_minutes = int((end_dt - start_dt).total_seconds() / 60)
                else:
                    duration_minutes = 0
            else:
                duration_minutes = 0
            
            return {
                "callsign": callsign,
                "track_points": track_points,
                "total_points": len(track_points),
                "flight_duration_minutes": duration_minutes
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting flight track for {callsign}: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting flight track: {str(e)}")

@app.get("/api/flights/{callsign}/stats")
@handle_service_errors
@log_operation("get_flight_stats")
async def get_flight_stats(callsign: str):
    """Get flight statistics and summary information"""
    try:
        async with get_database_session() as session:
            # Get flight statistics
            stats_result = await session.execute(
                text("""
                    SELECT 
                        COUNT(*) as position_updates,
                        AVG(groundspeed) as avg_groundspeed,
                        MAX(altitude) as max_altitude,
                        MIN(last_updated) as first_update,
                        MAX(last_updated) as last_update
                    FROM flights 
                    WHERE callsign = :callsign
                """),
                {"callsign": callsign}
            )
            
            stats_row = stats_result.fetchone()
            if not stats_row or stats_row[0] == 0:
                raise HTTPException(status_code=404, detail=f"No flight data found for callsign '{callsign}'")
            
            # Calculate flight time
            first_update = stats_row[3]
            last_update = stats_row[4]
            if first_update and last_update:
                flight_time_minutes = int((last_update - first_update).total_seconds() / 60)
            else:
                flight_time_minutes = 0
            
            # Calculate route efficiency (placeholder)
            route_efficiency = 0.98  # Placeholder calculation
            
            return {
                "callsign": callsign,
                "statistics": {
                    "total_distance_nm": 0,  # Placeholder
                    "average_groundspeed": float(stats_row[1]) if stats_row[1] else 0,
                    "max_altitude": stats_row[2] if stats_row[2] else 0,
                    "flight_time_minutes": flight_time_minutes,
                    "position_updates": stats_row[0],
                    "route_efficiency": route_efficiency
                }
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting flight stats for {callsign}: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting flight stats: {str(e)}")

@app.get("/api/flights/memory")
@handle_service_errors
@log_operation("get_memory_flights")
async def get_memory_flights():
    """Get flights from memory buffer (debugging endpoint)"""
    try:
        # Get data service for memory buffer status
        data_service = await get_data_service()
        # Get buffer status (simplified)
        buffer_status = {"status": "operational"}
        
        return {
            "memory_flights": buffer_status.get('flights_count', 0),
            "cache_status": "disabled",  # Cache removed
            "last_refresh": datetime.now(timezone.utc).isoformat(),
            "flights": []  # Memory buffer data not exposed via API
        }
        
    except Exception as e:
        logger.error(f"Error getting memory flights: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting memory flights: {str(e)}")

# ATC Controller Endpoints

@app.get("/api/controllers")
@handle_service_errors
@log_operation("get_all_controllers")
async def get_all_controllers():
    """Get all active ATC positions"""
    try:
        async with get_database_session() as session:
            # Get recent ATC positions (last 30 minutes)
            recent_cutoff = datetime.now(timezone.utc) - timedelta(minutes=30)
            
            controllers_result = await session.execute(
                text("""
                    SELECT DISTINCT ON (callsign) 
                        callsign, cid, name, facility, rating, server,
                        visual_range, text_atis, logon_time, last_updated
                    FROM controllers 
                    WHERE last_updated >= :cutoff
                    ORDER BY callsign, last_updated DESC
                """),
                {"cutoff": recent_cutoff}
            )
            
            controllers = []
            for row in controllers_result.fetchall():
                controller = {
                    "callsign": row[0],
                    "cid": row[1],
                    "name": row[2],
                    "facility": row[3],
                    "rating": row[4],
                    "server": row[5],
                    "visual_range": row[6],
                    "text_atis": row[7],
                    "logon_time": row[8].isoformat() if row[8] else None,
                    "last_updated": row[9].isoformat() if row[9] else None
                }
                controllers.append(controller)
            
            return {
                "controllers": controllers,
                "total_count": len(controllers),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error getting controllers: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting controllers: {str(e)}")

@app.get("/api/atc-positions")
@handle_service_errors
@log_operation("get_atc_positions")
async def get_atc_positions():
    """Alternative endpoint for ATC positions (legacy compatibility)"""
    return await get_all_controllers()

@app.get("/api/atc-positions/by-controller-id")
@handle_service_errors
@log_operation("get_atc_positions_by_controller_id")
async def get_atc_positions_by_controller_id():
    """Get ATC positions grouped by controller ID"""
    try:
        async with get_database_session() as session:
            # Get recent ATC positions grouped by controller ID (last 24 hours)
            recent_cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
            
            controllers_result = await session.execute(
                text("""
                    SELECT DISTINCT ON (callsign) 
                        cid, callsign, facility, logon_time
                    FROM controllers 
                    WHERE last_updated >= :cutoff
                    ORDER BY callsign, last_updated DESC
                """),
                {"cutoff": recent_cutoff}
            )
            
            controllers_by_id = {}
            for row in controllers_result.fetchall():
                cid = str(row[0])
                if cid not in controllers_by_id:
                    controllers_by_id[cid] = []
                
                controller = {
                    "callsign": row[1],
                    "facility": row[2],
                    "logon_time": row[3].isoformat() if row[3] else None
                }
                controllers_by_id[cid].append(controller)
            
            return {
                "controllers_by_id": controllers_by_id
            }
            
    except Exception as e:
        logger.error(f"Error getting ATC positions by controller ID: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting ATC positions: {str(e)}")

# VATSIM Ratings Endpoint

@app.get("/api/vatsim/ratings")
@handle_service_errors
@log_operation("get_vatsim_ratings")
async def get_vatsim_ratings():
    """Get VATSIM controller ratings and descriptions"""
    try:
        # Define VATSIM ratings (static data)
        ratings = {
            "1": "Observer",
            "2": "Student 1", 
            "3": "Student 2",
            "4": "Student 3",
            "5": "Controller 1",
            "7": "Controller 3",
            "8": "Instructor 1",
            "10": "Instructor 3"
        }
        
        return {
            "ratings": ratings,
            "total_ratings": len(ratings),
            "description": "VATSIM controller ratings from 1-15",
            "valid_range": "1-15",
            "known_ratings": [1, 2, 3, 4, 5, 8, 10, 11],
            "unknown_ratings": [6, 7, 9, 12, 13, 14, 15]
        }
        
    except Exception as e:
        logger.error(f"Error getting VATSIM ratings: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting VATSIM ratings: {str(e)}")



@app.get("/api/filter/boundary/status")
@handle_service_errors
@log_operation("get_boundary_filter_status")
async def get_boundary_filter_status():
    """Get geographic boundary filter status and performance"""
    try:
        # Get boundary filter status
        data_service = await get_data_service()
        boundary_filter = data_service.geographic_boundary_filter
        
        # Get filter statistics
        filter_stats = boundary_filter.get_filter_stats()
        
        return {
            "boundary_filter": {
                "enabled": boundary_filter.config.enabled,
                "status": "ready" if boundary_filter.is_initialized else "uninitialized",
                "performance": {
                    "average_processing_time_ms": filter_stats.get("processing_time_ms", 0),
                    "total_calculations": filter_stats.get("total_calculations", 0),
                    "cache_hits": 0,  # Cache removed
                    "cache_misses": 0  # Cache removed
                },
                "configuration": {
                    "boundary_data_path": boundary_filter.config.boundary_data_path,
                    "log_level": boundary_filter.config.log_level,
                    "polygon_loaded": boundary_filter.is_initialized,
                    "polygon_points": filter_stats.get("polygon_points", 0)
                }
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting boundary filter status: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting boundary filter status: {str(e)}")

@app.get("/api/filter/boundary/info")
@handle_service_errors
@log_operation("get_boundary_filter_info")
async def get_boundary_filter_info():
    """Get boundary polygon information and configuration"""
    try:
        # Get boundary filter info
        data_service = await get_data_service()
        boundary_filter = data_service.geographic_boundary_filter
        
        if not boundary_filter.is_initialized:
            raise HTTPException(status_code=503, detail="Boundary filter not initialized")
        
        # Get polygon information
        polygon_info = boundary_filter.get_boundary_info()
        
        return {
            "boundary_info": polygon_info
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting boundary filter info: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting boundary filter info: {str(e)}")

@app.get("/api/filter/controller-callsign/status")
@handle_service_errors
@log_operation("get_controller_callsign_filter_status")
async def get_controller_callsign_filter_status():
    """Get controller callsign filter status and performance"""
    try:
        # Get controller callsign filter status
        data_service = await get_data_service()
        controller_filter = data_service.controller_callsign_filter
        
        # Get filter statistics
        filter_stats = controller_filter.get_filter_stats()
        filter_status = controller_filter.get_filter_status()
        
        return {
            "controller_callsign_filter": {
                "enabled": filter_status["enabled"],
                "status": "active" if filter_status["filtering_active"] else "inactive",
                "performance": {
                    "total_processed": filter_stats.get("total_processed", 0),
                    "controllers_included": filter_stats.get("controllers_included", 0),
                    "controllers_excluded": filter_stats.get("controllers_excluded", 0),
                    "filtering_rate": f"{filter_stats.get('controllers_excluded', 0) / max(filter_stats.get('total_processed', 1), 1) * 100:.1f}%" if filter_stats.get('total_processed', 0) > 0 else "0%"
                },
                "configuration": {
                    "callsign_list_path": filter_status["callsign_list_path"],
                    "valid_callsigns_loaded": filter_status["valid_callsigns_loaded"],
                    "case_sensitive": filter_status["case_sensitive"],
                    "filtering_active": filter_status["filtering_active"]
                },
                "sample_callsigns": controller_filter.get_valid_callsigns_sample(5)
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting controller callsign filter status: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting controller callsign filter status: {str(e)}")

@app.post("/api/filter/controller-callsign/reload")
@handle_service_errors
@log_operation("reload_controller_callsign_filter")
async def reload_controller_callsign_filter():
    """Reload the controller callsign list from file"""
    try:
        # Get controller callsign filter and reload
        data_service = await get_data_service()
        controller_filter = data_service.controller_callsign_filter
        
        # Attempt to reload callsigns
        success = controller_filter.reload_callsigns()
        
        if success:
            # Get updated status
            filter_status = controller_filter.get_filter_status()
            return {
                "message": "Controller callsign filter reloaded successfully",
                "status": "success",
                "callsigns_loaded": filter_status["valid_callsigns_loaded"],
                "filtering_active": filter_status["filtering_active"]
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to reload controller callsign filter")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error reloading controller callsign filter: {e}")
        raise HTTPException(status_code=500, detail=f"Error reloading controller callsign filter: {str(e)}")

# Analytics Endpoints

@app.get("/api/analytics/flights")
@handle_service_errors
@log_operation("get_flight_analytics")
async def get_flight_analytics():
    """Get flight summary data and analytics - simplified"""
    try:
        async with get_database_session() as session:
            result = await session.execute(
                text("""
                    SELECT 
                        COUNT(DISTINCT callsign) as flights,
                        COUNT(*) as positions,
                        AVG(groundspeed) as avg_speed,
                        MAX(altitude) as max_alt,
                        COUNT(DISTINCT departure) as deps,
                        COUNT(DISTINCT arrival) as arrs
                    FROM flights 
                    WHERE last_updated >= NOW() - INTERVAL '24 hours'
                """)
            )
            
            row = result.fetchone()
            return {
                "flights": row[0] or 0,
                "positions": row[1] or 0,
                "avg_speed": float(row[2]) if row[2] else 0,
                "max_alt": row[3] or 0,
                "departures": row[4] or 0,
                "arrivals": row[5] or 0,
                "period": "24h"
            }
            
    except Exception as e:
        logger.error(f"Flight analytics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Performance & Monitoring Endpoints

@app.get("/api/performance/metrics")
@handle_service_errors
@log_operation("get_performance_metrics")
async def get_performance_metrics():
    """Get basic system status - simplified"""
    return {
        "status": "operational",
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.post("/api/performance/optimize")
@handle_service_errors
@log_operation("trigger_performance_optimization")
async def trigger_performance_optimization():
    """System status check - simplified"""
    return {
        "status": "operational",
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

# Transceiver Data Endpoints

@app.get("/api/transceivers")
@handle_service_errors
@log_operation("get_transceivers")
async def get_transceivers():
    """Get radio frequency and position data"""
    try:
        async with get_database_session() as session:
            # Get recent transceivers (last 5 minutes for fresh data)
            recent_cutoff = datetime.now(timezone.utc) - timedelta(minutes=5)
            
            transceivers_result = await session.execute(
                text("""
                    SELECT DISTINCT ON (callsign) 
                        id, callsign, frequency, position_lat, position_lon, height_msl, timestamp
                    FROM transceivers 
                    WHERE timestamp >= :cutoff
                    ORDER BY callsign, timestamp DESC
                """),
                {"cutoff": recent_cutoff}
            )
            
            transceivers = []
            for row in transceivers_result.fetchall():
                transceiver = {
                    "id": row[0],
                    "callsign": row[1],
                    "frequency": row[2],
                    "position_lat": row[3],
                    "position_lng": row[4],
                    "height_msl": row[5],
                    "timestamp": row[6].isoformat() if row[6] else None
                }
                transceivers.append(transceiver)
            
            return {
                "transceivers": transceivers,
                "total_count": len(transceivers)
            }
            
    except Exception as e:
        logger.error(f"Error getting transceivers: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting transceivers: {str(e)}")

# Database Operations Endpoints

@app.get("/api/database/tables")
@handle_service_errors
@log_operation("get_database_tables")
async def get_database_tables():
    """Get database tables and record counts - simplified"""
    try:
        async with get_database_session() as session:
            result = await session.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public'
                ORDER BY table_name
            """))
            
            tables = []
            for row in result.fetchall():
                count_result = await session.execute(
                    text(f"SELECT COUNT(*) FROM {row[0]}")
                )
                tables.append({
                    "name": row[0],
                    "count": count_result.scalar() or 0
                })
            
            return {"tables": tables}
            
    except Exception as e:
        logger.error(f"Database tables error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


class DatabaseQueryRequest(BaseModel):
    query: str
    limit: int = 1000

@app.post("/api/database/query")
@handle_service_errors
@log_operation("execute_database_query")
async def execute_database_query(request: DatabaseQueryRequest):
    """Execute custom SQL queries (admin only) - simplified"""
    # Basic query validation
    dangerous = ['DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE', 'INSERT', 'UPDATE']
    if any(keyword in request.query.upper() for keyword in dangerous):
        raise HTTPException(status_code=400, detail="Dangerous query")
    
    # Additional validation
    if not request.query.strip():
        raise HTTPException(status_code=400, detail="Query cannot be empty")
    
    if request.limit < 1 or request.limit > 10000:
        raise HTTPException(status_code=400, detail="Limit must be between 1 and 10000")
    
    try:
        async with get_database_session() as session:
            result = await session.execute(text(f"{request.query} LIMIT {request.limit}"))
            rows = result.fetchall()
            columns = result.keys()
            
            # Convert to list of dicts
            data = []
            for row in rows:
                row_dict = {}
                for i, column in enumerate(columns):
                    value = row[i]
                    if hasattr(value, 'isoformat'):
                        value = value.isoformat()
                    row_dict[column] = value
                data.append(row_dict)
            
            return {"results": data, "count": len(data)}
            
    except Exception as e:
        logger.error(f"Query error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Controller Summary Endpoints

@app.get("/api/controller-summaries")
@handle_service_errors
@log_operation("get_controller_summaries")
async def get_controller_summaries(
    callsign: Optional[str] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    limit: int = 100,
    offset: int = 0
):
    """Get controller summaries with optional filtering."""
    try:
        async with get_database_session() as session:
            # Build base query
            query = "SELECT * FROM controller_summaries WHERE 1=1"
            params = {}
            
            if callsign:
                query += " AND callsign ILIKE :callsign"
                params["callsign"] = f"%{callsign}%"
            
            if date_from:
                try:
                    date_from_parsed = datetime.fromisoformat(date_from.replace('Z', '+00:00'))
                    query += " AND session_start_time >= :date_from"
                    params["date_from"] = date_from_parsed
                except ValueError:
                    raise HTTPException(status_code=400, detail=f"Invalid date_from format: {date_from}")
            
            if date_to:
                try:
                    date_to_parsed = datetime.fromisoformat(date_to.replace('Z', '+00:00'))
                    query += " AND session_end_time <= :date_to"
                    params["date_to"] = date_to_parsed
                except ValueError:
                    raise HTTPException(status_code=400, detail=f"Invalid date_to format: {date_to}")
            
            # Add ordering and pagination
            query += " ORDER BY session_start_time DESC LIMIT :limit OFFSET :offset"
            params["limit"] = limit
            params["offset"] = offset
            
            # Execute query
            result = await session.execute(text(query), params)
            records = result.fetchall()
            
            # Convert to JSON-serializable format
            summaries = []
            for record in records:
                summary = {
                    "id": record.id,
                    "callsign": record.callsign,
                    "cid": record.cid,
                    "name": record.name,
                    "session_start_time": record.session_start_time.isoformat() if record.session_start_time else None,
                    "session_end_time": record.session_end_time.isoformat() if record.session_end_time else None,
                    "session_duration_minutes": record.session_duration_minutes,
                    "rating": record.rating,
                    "facility": record.facility,
                    "server": record.server,
                    "total_aircraft_handled": record.total_aircraft_handled,
                    "peak_aircraft_count": record.peak_aircraft_count,
                    "hourly_aircraft_breakdown": record.hourly_aircraft_breakdown,
                    "frequencies_used": record.frequencies_used,
                    "aircraft_details": record.aircraft_details,
                    "created_at": record.created_at.isoformat() if record.created_at else None,
                    "updated_at": record.updated_at.isoformat() if record.updated_at else None
                }
                summaries.append(summary)
            
            return {
                "summaries": summaries,
                "total": len(summaries),
                "limit": limit,
                "offset": offset,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error fetching controller summaries: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/controller-summaries/{callsign}/stats")
@handle_service_errors
@log_operation("get_controller_stats")
async def get_controller_stats(callsign: str):
    """Get statistics for a specific controller callsign."""
    try:
        async with get_database_session() as session:
            # Get summary statistics
            result = await session.execute(text("""
                SELECT 
                    COUNT(*) as total_sessions,
                    AVG(session_duration_minutes) as avg_session_duration,
                    SUM(total_aircraft_handled) as total_aircraft_handled,
                    MAX(peak_aircraft_count) as max_peak_aircraft,
                    MIN(session_start_time) as first_session,
                    MAX(session_start_time) as last_session
                FROM controller_summaries 
                WHERE callsign = :callsign
            """), {"callsign": callsign})
            
            stats = result.fetchone()
            if not stats:
                raise HTTPException(status_code=404, detail="Controller not found")
            
            return {
                "callsign": callsign,
                "total_sessions": stats.total_sessions,
                "avg_session_duration_minutes": float(stats.avg_session_duration) if stats.avg_session_duration else 0,
                "total_aircraft_handled": stats.total_aircraft_handled or 0,
                "max_peak_aircraft": stats.max_peak_aircraft or 0,
                "first_session": stats.first_session.isoformat() if stats.first_session else None,
                "last_session": stats.last_session.isoformat() if stats.last_session else None
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error fetching controller stats for {callsign}: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/controller-summaries/performance/overview")
@handle_service_errors
@log_operation("get_performance_overview")
async def get_performance_overview():
    """Get overall performance overview of controller summaries."""
    try:
        async with get_database_session() as session:
            # Get performance metrics
            result = await session.execute(text("""
                SELECT 
                    COUNT(*) as total_summaries,
                    AVG(session_duration_minutes) as avg_session_duration,
                    SUM(total_aircraft_handled) as total_aircraft_handled,
                    AVG(total_aircraft_handled) as avg_aircraft_per_session,
                    MAX(peak_aircraft_count) as max_peak_aircraft,
                    COUNT(DISTINCT callsign) as unique_controllers,
                    MIN(session_start_time) as earliest_session,
                    MAX(session_start_time) as latest_session
                FROM controller_summaries
            """))
            
            metrics = result.fetchone()
            
            return {
                "total_summaries": metrics.total_summaries or 0,
                "avg_session_duration_minutes": float(metrics.avg_session_duration) if metrics.avg_session_duration else 0,
                "total_aircraft_handled": metrics.total_aircraft_handled or 0,
                "avg_aircraft_per_session": float(metrics.avg_aircraft_per_session) if metrics.avg_aircraft_per_session else 0,
                "max_peak_aircraft": metrics.max_peak_aircraft or 0,
                "unique_controllers": metrics.unique_controllers or 0,
                "earliest_session": metrics.earliest_session.isoformat() if metrics.earliest_session else None,
                "latest_session": metrics.latest_session.isoformat() if metrics.latest_session else None
            }
            
    except Exception as e:
        logger.error(f"Error fetching performance overview: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/api/controller-summaries/process")
@handle_service_errors
@log_operation("trigger_controller_processing")
async def trigger_controller_processing():
    """Manually trigger controller summary processing."""
    try:
        data_service = await get_data_service()
        
        # Process completed controllers
        result = await data_service.process_completed_controllers()
        
        return {
            "status": "success",
            "message": "Controller summary processing completed",
            "result": result,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error triggering controller processing: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/api/flights/archive/populate-summary")
@handle_service_errors
@log_operation("populate_flights_archive_summary")
async def populate_flights_archive_summary():
    """Manually populate summary fields in flights_archive from flight_summaries."""
    try:
        data_service = await get_data_service()
        
        # Populate summary fields
        result = await data_service.populate_flights_archive_summary_fields()
        
        return {
            "status": "success",
            "message": "Flights archive summary fields populated",
            "records_updated": result,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error populating flights archive summary fields: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/flights/archive/sync-status")
@handle_service_errors
@log_operation("check_flights_archive_sync_status")
async def check_flights_archive_sync_status():
    """Check synchronization status between flight_summaries and flights_archive."""
    try:
        async with get_database_session() as session:
            # Check how many flights_archive records have summary data
            result = await session.execute(text("""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(deptime) as with_deptime,
                    COUNT(controller_callsigns) as with_controller_callsigns,
                    COUNT(controller_time_percentage) as with_controller_time,
                    COUNT(time_online_minutes) as with_time_online,
                    COUNT(primary_enroute_sector) as with_primary_sector,
                    COUNT(total_enroute_sectors) as with_total_sectors,
                    COUNT(total_enroute_time_minutes) as with_total_time,
                    COUNT(sector_breakdown) as with_sector_breakdown,
                    COUNT(completion_time) as with_completion_time
                FROM flights_archive
            """))
            
            counts = result.fetchone()
            
            # Calculate completion percentages
            total = counts.total_records or 0
            if total == 0:
                return {
                    "status": "no_data",
                    "message": "No records in flights_archive table",
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
            
            completion_rates = {
                "deptime": round((counts.with_deptime or 0) / total * 100, 2),
                "controller_callsigns": round((counts.with_controller_callsigns or 0) / total * 100, 2),
                "controller_time_percentage": round((counts.with_controller_time or 0) / total * 100, 2),
                "time_online_minutes": round((counts.with_time_online or 0) / total * 100, 2),
                "primary_enroute_sector": round((counts.with_primary_sector or 0) / total * 100, 2),
                "total_enroute_sectors": round((counts.with_total_sectors or 0) / total * 100, 2),
                "total_enroute_time_minutes": round((counts.with_total_time or 0) / total * 100, 2),
                "sector_breakdown": round((counts.with_sector_breakdown or 0) / total * 100, 2),
                "completion_time": round((counts.with_completion_time or 0) / total * 100, 2)
            }
            
            overall_completion = round(sum(completion_rates.values()) / len(completion_rates), 2)
            
            return {
                "status": "success",
                "total_records": total,
                "overall_completion_percentage": overall_completion,
                "field_completion_rates": completion_rates,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error checking flights archive sync status: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/health/controller-summary")
@handle_service_errors
@log_operation("health_controller_summary")
async def health_controller_summary():
    """Health check for controller summary system."""
    try:
        async with get_database_session() as session:
            # Check if tables exist and are accessible
            result = await session.execute(text("""
                SELECT 
                    (SELECT COUNT(*) FROM controller_summaries) as summaries_count,
                    (SELECT COUNT(*) FROM controllers_archive) as archive_count,
                    (SELECT COUNT(*) FROM controllers) as active_controllers
            """))
            
            counts = result.fetchone()
            
            return {
                "status": "healthy",
                "tables": {
                    "controller_summaries": counts.summaries_count or 0,
                    "controllers_archive": counts.archive_count or 0,
                    "controllers": counts.active_controllers or 0
                },
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Controller summary health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/api/dashboard/controller-summaries")
@handle_service_errors
@log_operation("dashboard_controller_summaries")
async def dashboard_controller_summaries():
    """Get dashboard data for controller summaries."""
    try:
        async with get_database_session() as session:
            # Get recent activity
            recent_result = await session.execute(text("""
                SELECT 
                    callsign,
                    session_start_time,
                    session_duration_minutes,
                    total_aircraft_handled,
                    peak_aircraft_count
                FROM controller_summaries 
                ORDER BY session_start_time DESC 
                LIMIT 10
            """))
            
            recent_sessions = []
            for record in recent_result.fetchall():
                recent_sessions.append({
                    "callsign": record.callsign,
                    "session_start": record.session_start_time.isoformat() if record.session_start_time else None,
                    "duration_minutes": record.session_duration_minutes,
                    "aircraft_handled": record.total_aircraft_handled,
                    "peak_aircraft": record.peak_aircraft_count
                })
            
            # Get top controllers by aircraft handled
            top_result = await session.execute(text("""
                SELECT 
                    callsign,
                    COUNT(*) as sessions,
                    SUM(total_aircraft_handled) as total_aircraft,
                    AVG(session_duration_minutes) as avg_duration
                FROM controller_summaries 
                GROUP BY callsign 
                ORDER BY total_aircraft DESC 
                LIMIT 5
            """))
            
            top_controllers = []
            for record in top_result.fetchall():
                top_controllers.append({
                    "callsign": record.callsign,
                    "sessions": record.sessions,
                    "total_aircraft": record.total_aircraft or 0,
                    "avg_duration_minutes": float(record.avg_duration) if record.avg_duration else 0
                })
            
            return {
                "recent_sessions": recent_sessions,
                "top_controllers": top_controllers,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
    except Exception as e:
        logger.error(f"Error fetching dashboard data: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")

# Root endpoint
@app.get("/")
async def root():
    """Root endpoint - simplified"""
    return {
        "status": "operational",
        "version": "1.0.0"
    }

@app.get("/api/startup/health")
async def startup_health_check():
    """Startup health check - verifies all required database tables exist"""
    try:
        async with get_database_session() as session:
            # Get all required tables
            required_tables = [
                'flights', 'controllers', 'transceivers', 'flight_summaries', 
                'flights_archive', 'flight_sector_occupancy', 'controller_summaries', 
                'controllers_archive'
            ]
            
            # Query existing tables
            existing_tables_result = await session.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public'
                ORDER BY table_name
            """))
            existing_tables = [row[0] for row in existing_tables_result.fetchall()]
            
            # Check each required table
            table_status = {}
            missing_tables = []
            
            for table in required_tables:
                if table in existing_tables:
                    # Verify table is accessible by doing a simple count
                    try:
                        count_result = await session.execute(text(f"SELECT COUNT(*) FROM {table}"))
                        count = count_result.scalar()
                        table_status[table] = {
                            "exists": True,
                            "accessible": True,
                            "record_count": count or 0,
                            "status": "healthy"
                        }
                    except Exception as e:
                        table_status[table] = {
                            "exists": True,
                            "accessible": False,
                            "error": str(e),
                            "status": "unhealthy"
                        }
                else:
                    table_status[table] = {
                        "exists": False,
                        "accessible": False,
                        "status": "missing"
                    }
                    missing_tables.append(table)
            
            # Determine overall health
            if missing_tables:
                overall_status = "critical"
                status_message = f"Missing {len(missing_tables)} required tables: {', '.join(missing_tables)}"
            else:
                # Check if all tables are accessible
                inaccessible_tables = [table for table, status in table_status.items() 
                                    if not status.get("accessible", True)]
                if inaccessible_tables:
                    overall_status = "degraded"
                    status_message = f"{len(inaccessible_tables)} tables are inaccessible"
                else:
                    overall_status = "healthy"
                    status_message = "All required tables exist and are accessible"
            
            return {
                "startup_health": {
                    "overall_status": overall_status,
                    "status_message": status_message,
                    "required_tables_count": len(required_tables),
                    "existing_tables_count": len(existing_tables),
                    "missing_tables_count": len(missing_tables),
                    "missing_tables": missing_tables,
                    "table_status": table_status,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
            }
            
    except Exception as e:
        return {
            "startup_health": {
                "overall_status": "critical",
                "status_message": f"Failed to check database health: {str(e)}",
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        }
